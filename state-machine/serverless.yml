# To access resources defined in Terraform, we use SSM
# https://www.serverless.com/blog/definitive-guide-terraform-serverless/
#
# For full config options, check the docs:
#    docs.serverless.com
#

service: mentorpal-upload-sm

# pin to only deploy with a specific Serverless version
frameworkVersion: '2'

variablesResolutionMode: 20210326

# todo install drugin build:
# sls plugin install -n serverless-python-requirements
# sls plugin install -n serverless-offline
# sls plugin install -n serverless-layers

plugins:
  - serverless-python-requirements
  - serverless-offline
  # - serverless-layers

custom:
  stages:
    offline:
      LOG_LEVEL: 'trace'
      S3_STATIC: ${ssm:/mentorpal/v2/s3_static_arn}
    dev:
      LOG_LEVEL: 'trace'
      # v2 is hardcoded, we could use fallback stages instead
      S3_STATIC: ${ssm:/mentorpal/v2/s3_static_arn}
    qa:
      # TODO
      # S3_STATIC: ${ssm:/mentorpal/${self:provider.stage}/s3_static_arn}
      LOG_LEVEL: 'debug'
    prod:
      LOG_LEVEL: 'info'
      # TODO
      # S3_STATIC: ${ssm:/mentorpal/${self:provider.stage}/s3_static_arn}
  # https://github.com/serverless/serverless-python-requirements
  pythonRequirements:
    dockerizePip: non-linux # instructs the python requirments plugin to use a docker container to package requirements

  # todo, replace pythonRequirements with serverless-layers
  # serverless-layers requires a deployment bucket to be created before deploying this stack (in terraform)
  # serverless-layers:
  #   - dependencies:
  #       layersDeploymentBucket: '${self:service}-dependencies-${self:provider.stage}'
  #       dependenciesPath: ./requirements.txt
  #       compatibleRuntimes:
  #         - python3.7
  #         - python3.8
  #       # applies to all lambdas

  # serverless-offline:
  #   useDocker: true

provider:
  name: aws
  stage: ${opt:stage, 'dev'} # stage is dev unless otherwise specified with --stage flag
  stackTags:
    ENVIRONMENT: ${self:provider.stage}
    PROJECT: ${self:service}-${self:provider.stage}
    REPOSITORY: mentor-upload
  runtime: python3.8
  lambdaHashingVersion: 20201221
  tracing:
    lambda: true
  logRetentionInDays: 30      
  region: us-east-1
  environment:
    # IS_OFFLINE: ${env:IS_OFFLINE}
    STAGE: ${self:provider.stage}
    PYTHON_ENV: ${self:provider.stage}
  # iam permissions for all lambda functions
  iam:
    role:
      statements:
        - Effect: "Allow"
          Action:
            - "s3:PutObject"
            - "s3:GetObject"
          Resource:
            Fn::Join:
              - "/"
              - - ${self:custom.stages.${self:provider.stage}.S3_STATIC}
                - "*"
package:
#  individually: false
 patterns:
    # exclude everything:
     - '!./**'
    # and then add back in only the files we need:
     - trim.py
    #  - ./binaries/ffmpeg/ffmpeg
    #  - './binaries/MediaInfo_DLL_21.09_Lambda/lib/**'
     - requirements.txt
     - celery-short.mp4

layers:
  # binaries are shared and this will make lambdas size smaller
  binaries:
    path: ./binaries
    package:
      patterns:
        - '!./**'
        # relative to layer path:
        # when attached, these are available under /opt
        - ./ffmpeg/ffmpeg
        - './MediaInfo_DLL_21.09_Lambda/lib/**'    
    name: binaries-layer-${self:provider.stage}
    description: Bundles ffmpeg and mediainfo binaries/libs
    compatibleArchitectures: # optional, a list of architectures this layer is compatible with
      - x86_64
    licenseInfo: GPLv3 # optional, a string specifying license information
    retain: false # If true, layer versions are not deleted as new ones are created

functions:
  step-trim:
    handler: trim.handler
    memorySize: 2048 # todo benchmark to find the optimal size
    timeout: 300
    layers:
      # binaries gets named "Binaries"+"LambdaLayer":
      # see https://www.serverless.com/framework/docs/providers/aws/guide/layers#using-your-layers
      - { Ref: BinariesLambdaLayer }

#    The following are a few example events you can configure
#    NOTE: Please make sure to change your handler code to work with those events
#    Check the event documentation for details
#    events:
#      - httpApi:
#          path: /users/create
#          method: get
#      - websocket: $connect
#      - s3: ${env:BUCKET}
#      - schedule: rate(10 minutes)
#      - sns: greeter-topic
#      - stream: arn:aws:dynamodb:region:XXXXXX:table/foo/stream/1970-01-01T00:00:00.000
#      - alexaSkill: amzn1.ask.skill.xx-xx-xx-xx
#      - alexaSmartHome: amzn1.ask.skill.xx-xx-xx-xx
#      - iot:
#          sql: "SELECT * FROM 'some_topic'"
#      - cloudwatchEvent:
#          event:
#            source:
#              - "aws.ec2"
#            detail-type:
#              - "EC2 Instance State-change Notification"
#            detail:
#              state:
#                - pending
#      - cloudwatchLog: '/aws/lambda/hello'
#      - cognitoUserPool:
#          pool: MyUserPool
#          trigger: PreSignUp
#      - alb:
#          listenerArn: arn:aws:elasticloadbalancing:us-east-1:XXXXXX:listener/app/my-load-balancer/50dc6c495c0c9188/
#          priority: 1
#          conditions:
#            host: example.com
#            path: /hello

